# 10.2 Multimodal Large Model Applications

[TOC]



<p id ="anther10.2.1"></p>

## 10.2.1 Large Model API Key Setup

> [!NOTE]
>
> **This section requires registering on the official OpenAI website and obtaining an API key for accessing large language models.**

### 10.2.1.1 OpenAI Account Registration and Deployment

1) Copy and open the following URL: https://platform.openai.com/docs/overview, then click the **Sign Up** button in the upper-right corner.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image1.png"  />

2) Register and log in using a Google, Microsoft, or Apple account, as prompted.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image2.png" style="width:500px" />

3) After logging in, click the Settings button, then go to **Billing**, and click **Payment Methods** to add a payment method. The payment is used to purchase **tokens**.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image3.png"  />

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image4.png" style="width:700px" />

4) After completing the preparation steps, click **API Keys** and create a new key. Follow the prompts to fill in the required information, then save the key for later use.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image5.png"  />

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image6.png"  />

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image7.png"  />

5) The creation and deployment of the large model have been completed, and this API will be used in the following sections.

### 10.2.1.2 OpenRouter Account Registration and Deployment

1) Copy the URL https://openrouter.ai/ into a browser and open the webpage. Click **Sign in** and register or sign in using a Google account or another available account.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image8.png" style="width:700px" />

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image9.png"  />

2) After logging in, click the icon in the top-right corner, then select **Credits** to add a payment method.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image10.png" style="width:700px" />

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image11.png" style="width:700px" />

3) Create an API key. Go to **API Keys**, then click **Create Key**. Follow the prompts to generate a key. Save the API key securely for later use.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image12.png" style="width:700px" />

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image13.png" style="width:700px" />

The creation and deployment of the large model have been completed, and this API will be used in the following sections.

### 10.2.1.3 API Configuration

1. Click <img class="common_img" src="../_static/media/chapter_22/section_1/media/image25.png"   /> to open a terminal and enter the following command to open the configuration file. Press the i key to enter input mode.

```bash
vim /home/ubuntu/ros2_ws/src/large_models/large_models/large_models/config.py
```

2. Fill in the obtained Large Model API Key in the corresponding parameter, as shown in the red box in the figure below.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image15.png"  />

3. Press **Esc**, then enter the command and press **Enter** to save and exit the configuration file.

```bash
:wq
```

<p id ="anther10.2.2"></p>

## 10.2.2 Version Confirmation

Before starting features, verify that the correct microphone configuration is set in the system.

1. After remotely logging in via NoMachine, click the desktop icon <img class="common_img" src="../_static/media/chapter_22/section_1/media/image16.png"  /> to access the configuration interface.

2. Select the appropriate microphone version configuration according to the hardware.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image17.png"  />

3. If using the AI Voice Interaction Box, select **WonderEcho Pro** as the microphone type, as shown in the figure below.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image18.png"  style="width:300px" />

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image19.png"  />

4. For the 6-Microphone Array, select **xf** as the microphone type as shown in the figure.
   
   <img class="common_img" src="../_static/media/chapter_22/section_1/media/image20.png"  style="width:300px" />
   
   <img class="common_img" src="../_static/media/chapter_22/section_1/media/image21.png"  />
   
5. Click **Save**.
   
   <img class="common_img" src="../_static/media/chapter_22/section_1/media/image22.png"  />
   
6. After the **Save Success** notification appears, click **Apply**.
   
   <img class="common_img" src="../_static/media/chapter_22/section_1/media/image23.png"  />
   
7. Finally, click **Quit** to exit the software interface.
   
   <img class="common_img" src="../_static/media/chapter_22/section_1/media/image24.png"  />
   


## 10.2.3 Voice Control

### 10.2.3.1 Program Overview

Once the program starts, the voice device will announce “**I’m ready**”. Say the designated wake word **Hello Hiwonder** to activate the voice device, which will respond with “**I’m here**”. Then, the robot can be controlled via voice commands to perform corresponding actions, for example: “**Move forward, backward.**” The voice device will respond after processing the command and execute the corresponding action.

### 10.2.3.2 Preparation

* **Version Confirmation**

Before starting this feature, ensure that the microphone version configuration in the system is correct. For more details, refer to section [10.2.2 Version Confirmation](#anther10.2.2).

* **Configure the Large Model API Key**

Refer to the section [10.2.1 Large Model API Key Setup](#anther10.2.1) to set up the large model key.

### 10.2.3.3 Operation Steps

> [!NOTE]
> 
> * **Command input is case-sensitive and space-sensitive.**
> 
> * **The robot must be connected to the Internet, either in STA (LAN) mode or AP (direct connection) mode via Ethernet.**

1. Click the terminal <img class="common_img" src="../_static/media/chapter_22/section_1/media/image25.png"  /> on the left side of the system interface to open the command line. Enter the command and press **Enter** to disable the app auto-start service.

```bash
sudo systemctl stop start_app_node.service
```

2. Enter the following command and press **Enter** to launch the voice control feature.

```bash
ros2 launch large_models_examples llm_control_move.launch.py
```

3. When the command line displays the output shown below and announces “**I’m ready**”, the voice device has completed initialization. At this point, say the wake word **Hello Hiwonder**.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image26.png" style="width:700px" />

4. When the command line shows the output below and the voice device announces “**I’m here**”, the voice device has been successfully activated. The system will begin recording your voice command.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image27.png" style="width:700px" />

5. When the terminal displays the next output as the reference image, it shows the recognized speech transcribed by the device.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image28.png" style="width:700px" />

6. When the command line displays the output shown below, the cloud-based large language model has been successfully invoked. It processes the user’s command, provides a verbal response, and generates actions corresponding to the semantic meaning of the command.

The response is automatically generated by the large model. Only the semantic accuracy of the reply is guaranteed, while the wording and formatting may vary.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image28.png" style="width:700px" />

7. When the command line displays the output shown below, the current dialogue session has ended. Refer to Step 4 to reactivate the voice device with the wake word and start a new dialogue session.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image29.png" style="width:700px" />

8. To exit the feature, press **Ctrl+C** in the terminal. If the feature does not shut down immediately, press **Ctrl+C** multiple times. If it still fails to exit, open a new terminal window and run the following command to terminate all active ROS processes and related programs.

```bash
~/.stop_ros.sh
```

### 10.2.3.4 Program Outcome

Once the feature is activated, commands can be freely issued, for example: “**Move forward, backward**.” The robot will then move forward and backward accordingly.

### 10.2.3.5 Program Analysis

* **Launch File Analysis**

The program file is located at: **/home/ubuntu/ros2_ws/src/large_models_examples/large_models_examples/llm_control_move.launch.py**

Define `launch_setup` Function

```python
def launch_setup(context):
    mode = LaunchConfiguration('mode', default=1)
    mode_arg = DeclareLaunchArgument('mode', default_value=mode)

    controller_package_path = get_package_share_directory('controller')
    large_models_package_path = get_package_share_directory('large_models')

    controller_launch = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            os.path.join(controller_package_path, 'launch/controller.launch.py')),
    )

    large_models_launch = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            os.path.join(large_models_package_path, 'launch/start.launch.py')),
        launch_arguments={'mode': mode}.items(),
    )


    llm_control_move_node = Node(
        package='large_models_examples',
        executable='llm_control_move',
        output='screen',
    )

    return [mode_arg,
            controller_launch,
            large_models_launch,
            llm_control_move_node,
            ]
```

This function is used to configure and launch actions in a Launch file.

`mode = LaunchConfiguration('mode', default=1)`: Defines a Launch parameter named mode with a default value of 1.

`mode_arg = DeclareLaunchArgument('mode', default_value=mode)`: Declares the mode parameter and adds it to the Launch description.

`controller_path和large_models_package_path`: Paths to the shared directories of the `controller` and `large_models` packages, which control the robot’s movement.

`controller_launch`: Uses `IncludeLaunchDescription` to include the `controller.launch.py ` file from the `controller` package.

`large_models_launch`: Uses `IncludeLaunchDescription` to include the `start.launch.py` file from the `large_models` package and passes the mode parameter.

Finally, returns a list containing all Launch actions.

* **Python File Analysis**

The program file is located at: **ros2_ws/src/large_models_examples/large_models_examples/llm_control_move.py**

1\. Define Prompt Template (PROMPT)

```python
else:
    PROMPT = '''
**Role
You are an intelligent car that can be controlled via linear velocity on the x and y axes (in meters per second), and angular velocity on the z axis (in radians per second). The movement duration is controlled by t (in seconds).
Your job is to generate a corresponding instruction based on user input.

**Requirements
- Ensure valid velocity ranges:
Linear velocity: x, y ∈ [-1.0, 1.0] (negative values mean reverse direction)
Angular velocity: z ∈ [-1.0, 1.0] (negative: clockwise, positive: counterclockwise)
- Execute multiple actions sequentially, returning a list of movement instructions under the action field.
- Always append a stop command [0.0, 0.0, 0.0, 0.0] at the end to ensure the car halts.
- Default values:
x and y: 0.2
z: 1.0
t: 2.0
- For each action sequence, craft a short (5–10 characters), witty, and endlessly variable response to make interactions fun and engaging.
- Output only the final JSON result. No explanations, no extra output.
- Format:
{
  "action": [[x1, y1, z1, t1], [x2, y2, z2, t2], ..., [0.0, 0.0, 0.0, 0.0]],
  "response": "short response"
}
- Possess strong mathematical reasoning to interpret and compute physical quantities like distance, time, and velocity.

## Special Notes
The "action" key should contain an array of stringified movement instructions in execution order. If no valid command is found, output an empty array [].
The "response" key should contain a creatively written, concise reply that matches the required tone and length.

**Examples
Input: Move forward for 2 seconds, then rotate clockwise for 1 second
Output:
{
  "action": [[0.2, 0.0, 0.0, 2.0], [0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0]],
  "response": "Full speed ahead, spin and go!"
}

Input: Move forward 1 meter
Output:
{
  "action": [[0.2, 0.0, 0.0, 5.0], [0.0, 0.0, 0.0, 0.0]],
  "response": "Got it!"
}
    '''
```

2\. Class Initialization

```python
class LLMControlMove(Node):
    def __init__(self, name):
        rclpy.init()
        super().__init__(name)
        
        self.action = []
        self.llm_result = ''
        self.running = True
        self.interrupt = False
        self.action_finish = False
        self.play_audio_finish = False
        
        timer_cb_group = ReentrantCallbackGroup()
        self.tts_text_pub = self.create_publisher(String, 'tts_node/tts_text', 1)
        self.create_subscription(String, 'agent_process/result', self.llm_result_callback, 1)
        self.create_subscription(Bool, 'vocal_detect/wakeup', self.wakeup_callback, 1, callback_group=timer_cb_group)
        self.create_subscription(Bool, 'tts_node/play_finish', self.play_audio_finish_callback, 1, callback_group=timer_cb_group)
        self.set_model_client = self.create_client(SetModel, 'agent_process/set_model')
        self.set_model_client.wait_for_service()

        self.awake_client = self.create_client(SetBool, 'vocal_detect/enable_wakeup')
        self.awake_client.wait_for_service()
        self.set_mode_client = self.create_client(SetInt32, 'vocal_detect/set_mode')
        self.set_mode_client.wait_for_service()
        self.set_prompt_client = self.create_client(SetString, 'agent_process/set_prompt')
        self.set_prompt_client.wait_for_service()
        self.mecanum_pub = self.create_publisher(Twist, '/controller/cmd_vel', 1)

        self.timer = self.create_timer(0.0, self.init_process, callback_group=timer_cb_group)
```

Set state variables such as action list `action`, interrupt flag `interrupt`, and action completion flag `action_finish`. Create publishers such as TTS text publisher and subscribers such as large model results, TTS playback completion signal, and wake-up signal. Create service clients, such as for configuring the large model or enabling voice for communication and control.

3\. Set LLM Model

```python
def init_process(self):
    self.timer.cancel()

    msg = SetModel.Request()
    # msg.model = 'qwen-plus-latest'
    msg.model = llm_model
    msg.model_type = 'llm'
    msg.api_key = api_key 
    msg.base_url = base_url
    self.send_request(self.set_model_client, msg)

    msg = SetString.Request()
    msg.data = PROMPT
    self.send_request(self.set_prompt_client, msg)

    speech.play_audio(start_audio_path) 
    threading.Thread(target=self.process, daemon=True).start()
    self.create_service(Empty, '~/init_finish', self.get_node_state)
    self.get_logger().info('\033[1;32m%s\033[0m' % 'start')
    self.get_logger().info('\033[1;32m%s\033[0m' % PROMPT)
```

4\. `play_audio_finish_callback` Method

```python
def play_audio_finish_callback(self, msg):
  msg = SetBool.Request()
  msg.data = True
  self.send_request(self.awake_client, msg)
  # msg = SetInt32.Request()
  # msg.data = 1
  # self.send_request(self.set_mode_client, msg)
  self.play_audio_finish = msg.data
```

The method handles the callback after audio playback and re-enables the voice wake-up functionality.

5\. `process` Method

```python
def process(self):
    while self.running:
        if self.llm_result:
            msg = String()
            if 'action' in self.llm_result:  # If there is a corresponding behavior returned, extract and process it.
                result = eval(self.llm_result[self.llm_result.find('{'):self.llm_result.find('}') + 1])
                self.get_logger().info(str(result))
                action_list = []
                if 'action' in result:
                    action_list = result['action']
                if 'response' in result:
                    response = result['response']
                msg.data = response
                self.tts_text_pub.publish(msg)
                for i in action_list:
                    msg = Twist()
                    msg.linear.x = float(i[0])
                    msg.linear.y = float(i[1])
                    msg.angular.z = float(i[2])
                    self.mecanum_pub.publish(msg)
                    time.sleep(i[3])
                    if self.interrupt:
                        self.interrupt = False
                        self.mecanum_pub.publish(Twist())
                        break
            else:  # If there is no corresponding behavior, only respond.
                response = self.llm_result
                msg.data = response
                self.tts_text_pub.publish(msg)
            self.action_finish = True 
            self.llm_result = ''
        else:
            time.sleep(0.01)
        if self.play_audio_finish and self.action_finish:
            self.play_audio_finish = False
            self.action_finish = False
            # msg = SetInt32.Request()
            # msg.data = 2
            # self.send_request(self.set_mode_client, msg)
    rclpy.shutdown()
```

The main loop handles instructions from the LLM, parses commands, executes the corresponding actions, and provides voice feedback.



## 10.2.4 Autonomous Patrolling

### 10.2.3.1 Program Overview

When the program starts, the voice device will announce “**I'm ready**.” To activate the voice device, say the designated wake word **Hello Hiwonder**. Upon successful activation, the voice device will respond with “**I’m here**.” Once activated, voice commands can be issued, for example, “**Follow the black line and stop when you encounter an obstacle.**” The terminal displays the recognized command, and the voice device provides a processed response. The robot then follows the black line detected by its camera and stops automatically when an obstacle is detected.

### 10.2.3.2 Preparation

* **Version Confirmation**

Before starting this feature, ensure that the microphone version configuration in the system is correct. For more details, refer to section [10.2.2 Version Confirmation](#anther10.2.2).

* **Configure the Large Model API Key**

Refer to the section [10.2.1 Large Model API Key Setup](#anther10.2.1) to set up the large model key.

### 10.2.3.3 Operation Steps

> [!NOTE]
> 
> * **Command input is case-sensitive and space-sensitive.**
> 
> * **The robot must be connected to the Internet, either in STA (LAN) mode or AP (direct connection) mode via Ethernet.**

1) Click the terminal <img class="common_img" src="../_static/media/chapter_22/section_1/media/image25.png"  /> on the left side of the system interface to open the command line. Enter the command and press **Enter** to disable the app auto-start service.

```bash
sudo systemctl stop start_app_node.service
```

2) Enter the following command and press **Enter** to launch the autonomous patrolling feature.

```bash
ros2 launch large_models_examples llm_visual_patrol.launch.py
```

3) When the command line displays the output shown below and announces “**I’m ready**”, the voice device has completed initialization. At this point, say the wake word **Hello Hiwonder**.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image30.png" style="width:700px" />

4) When the command line shows the output below and the voice device announces “**I’m here**”, the voice device has been successfully activated. The system will begin recording your voice command.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image31.png" style="width:700px" />

5) When the terminal prints the output shown below, it indicates that the voice device has transcribed the spoken command. In this example, the command spoken is “**Follow the black line and stop when you encounter an obstacle**.”

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image32.png" style="width:700px" />

6) When the command line displays the output shown below, the cloud-based large language model has been successfully invoked. It processes the user’s command, provides a verbal response, and generates actions corresponding to the semantic meaning of the command.
   
   The response is automatically generated by the large model. Only the semantic accuracy of the reply is guaranteed, while the wording and formatting may vary.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image33.png" style="width:500px" />

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image34.png" style="width:700px" />

7) To stop the feature, refer to Step 4 to reactivate the voice device again with the wake word to begin a new interaction cycle.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image35.png" style="width:700px" />

8) To exit the feature, press **Ctrl+C** in the terminal. If the feature does not exit immediately, press **Ctrl+C** multiple times.

### 10.2.3.4 Program Outcome

Once the feature is activated, feel free to give commands in your own words. For instance, “**Follow the black line and stop when you encounter an obstacle**.” The robot uses its camera to detect and follow the black line, and it will stop when an obstacle is detected in its path. The system is pre-configured to recognize four line colors: red, blue, green, and black.

### 10.2.4.5 Program Analysis

* **Launch File Analysis**

The program file is located at: **/home/ubuntu/ros2_ws/src/large_models_examples/large_models_examples/llm_visual_patrol.launch.py**

Define `launch_setup` Function

```python
def launch_setup(context):
    mode = LaunchConfiguration('mode', default=1)
    mode_arg = DeclareLaunchArgument('mode', default_value=mode)

    app_package_path = get_package_share_directory('app')
    large_models_package_path = get_package_share_directory('large_models')

    line_following_node_launch = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            os.path.join(app_package_path, 'launch/line_following_node.launch.py')),
        launch_arguments={
            'debug': 'true',
        }.items(),
    )

    large_models_launch = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            os.path.join(large_models_package_path, 'launch/start.launch.py')),
        launch_arguments={'mode': mode}.items(),
    )

    llm_visual_patrol_node = Node(
        package='large_models_examples',
        executable='llm_visual_patrol',
        output='screen',
    )

    return [mode_arg,
            line_following_node_launch,
            large_models_launch,
            llm_visual_patrol_node,
            ]
```

This function is used to configure and launch actions in a Launch file.

`mode = LaunchConfiguration('mode', default=1)`: Defines a Launch parameter named mode with a default value of 1.

`mode_arg = DeclareLaunchArgument('mode', default_value=mode)`: Declares the mode parameter and adds it to the Launch description.

`large_models_package_path`: Retrieves the shared directory path of the `large_models` package.

`line_following_node_launch`: Uses `IncludeLaunchDescription` to include the `line_following_node.launch.py` file.

`large_models_launch`: Uses `IncludeLaunchDescription` to include the `start.launch.py` launch file from the `large_models` package and passes the mode parameter.

`llm_vision_pratrol`: Defines a ROS2 node from the `large_models` package that runs the `llm_vision_pratrol` executable and prints its output to the screen.

Finally, returns a list containing all Launch actions.

* **Python File Analysis**

The program file is located at: **/home/ubuntu/ros2_ws/src/large_models_examples/large_models_examples/llm_visual_patrol.py**

1\. Define Prompt Template (PROMPT)

```python
else:
    PROMPT = '''
**Role
You are a smart robot that generates corresponding JSON commands based on user input.

**Requirements
- For every user input, search for matching commands in the action function library and output the corresponding JSON instruction.
- For each action sequence, craft a witty and creative response (10 to 30 characters) to make interactions delightful.
- Directly return the JSON result — do not include any explanations or extra text.
- There are four target colors: red, green, blue, and black.
- Format:
{
  "action": ["xx", "xx"],
  "response": "xx"
}

**Special Notes
The "action" field should contain a list of function names as strings, ordered by execution. If no matching action is found, output an empty list: [].
The "response" field should provide a concise and charming reply, staying within the word and tone guidelines.

**Action Function Library
Follow a line of a given color: line_following('black')

**Example
Input: Follow the red line
Output:
{
  "action": ["line_following('red')"],
  "response": "Roger that!"
}
    '''
```

2\. Class Initialization

```python
def __init__(self, name):
    rclpy.init()
    super().__init__(name)

    self.action = []
    self.stop = True
    self.llm_result = ''
    # self.llm_result = '{"action": ["line_following(\'black\')"], "response": "ok！"}'
    self.running = True
    self.action_finish = False
    self.play_audio_finish = False

    timer_cb_group = ReentrantCallbackGroup()
    self.tts_text_pub = self.create_publisher(String, 'tts_node/tts_text', 1)
    self.create_subscription(String, 'agent_process/result', self.llm_result_callback, 1)
    self.create_subscription(Bool, 'vocal_detect/wakeup', self.wakeup_callback, 1, callback_group=timer_cb_group)
    self.create_subscription(Bool, 'tts_node/play_finish', self.play_audio_finish_callback, 1, callback_group=timer_cb_group)
    self.awake_client = self.create_client(SetBool, 'vocal_detect/enable_wakeup')
    self.awake_client.wait_for_service()
    self.set_mode_client = self.create_client(SetInt32, 'vocal_detect/set_mode')
    self.set_mode_client.wait_for_service()

    self.set_model_client = self.create_client(SetModel, 'agent_process/set_model')
    self.set_model_client.wait_for_service()
    self.set_prompt_client = self.create_client(SetString, 'agent_process/set_prompt')
    self.set_prompt_client.wait_for_service()
    self.enter_client = self.create_client(Trigger, 'line_following/enter')
    self.enter_client.wait_for_service()
    self.start_client = self.create_client(SetBool, 'line_following/set_running')
    self.start_client.wait_for_service()
    self.set_target_client = self.create_client(SetColor, 'line_following/set_color')
    self.set_target_client.wait_for_service()

    self.timer = self.create_timer(0.0, self.init_process, callback_group=timer_cb_group)
```

Set state variables, including stop flags, LLM processing outputs, and runtime status. Create publishers for TTS text output and subscribers for agent responses, wake-word signals, and audio-playback completion. Service-client requests cover entering tracking mode, updating the running state, and specifying the tracking color, providing the communication and control interface for the system.

3\. Set LLM Model

```python
def init_process(self):
    self.timer.cancel()

    msg = SetModel.Request()
    msg.model = llm_model
    msg.model_type = 'llm'
    msg.api_key = api_key 
    msg.base_url = base_url
    self.send_request(self.set_model_client, msg)

    msg = SetString.Request()
    msg.data = PROMPT
    self.send_request(self.set_prompt_client, msg)

    init_finish = self.create_client(Trigger, 'line_following/init_finish')
    init_finish.wait_for_service()
    self.send_request(self.enter_client, Trigger.Request())
    speech.play_audio(start_audio_path)
    threading.Thread(target=self.process, daemon=True).start()
    self.create_service(Empty, '~/init_finish', self.get_node_state)
    self.get_logger().info('\033[1;32m%s\033[0m' % 'start')
    self.get_logger().info('\033[1;32m%s\033[0m' % PROMPT)
```

4\. `play_audio_finish_callback` Method

```python
def play_audio_finish_callback(self, msg):
    self.play_audio_finish = msg.data
```

The method handles the callback after audio playback and re-enables the voice wake-up functionality.

5\. `process` Method

```python
def process(self):
    while self.running:
        if self.llm_result:
            msg = String()
            if 'action' in self.llm_result: # If there is a corresponding action returned, extract and process it(If there is a corresponding behavior returned, extract and process it.)
                result = eval(self.llm_result[self.llm_result.find('{'):self.llm_result.find('}')+1])
                if 'action' in result:
                    text = result['action']
                    # Use regular expressions to extract all strings inside parentheses.
                    pattern = r"line_following\('([^']+)'\)"
                    # Use re.search to find the matching result
                    for i in text:
                        match = re.search(pattern, i)
                        # Extract the result.
                        if match:
                            # Get all argument parts, precisely content inside parentheses.
                            color = match.group(1)
                            self.get_logger().info(str(color))
                            color_msg = SetColor.Request()
                            color_msg.data = color
                            self.send_request(self.set_target_client, color_msg)
                            # Start sorting.
                            start_msg = SetBool.Request()
                            start_msg.data = True 
                            self.send_request(self.start_client, start_msg)
                if 'response' in result:
                    msg.data = result['response']
            else: # If there is no corresponding action, just respond(If there is no corresponding behavior, only respond.)
                msg.data = self.llm_result
            self.tts_text_pub.publish(msg)
            self.action_finish = True
            self.llm_result = ''
        else:
            time.sleep(0.01)
        if self.play_audio_finish and self.action_finish:
            self.play_audio_finish = False
            self.action_finish = False
            msg = SetBool.Request()
            msg.data = True
            self.send_request(self.awake_client, msg)
            # msg = SetInt32.Request()
            # msg.data = 2
            # self.send_request(self.set_mode_client, msg)
            self.stop = False
    rclpy.shutdown()
```

By continuously monitoring the large model output, the system checks for any instruction that contains the `line_following` action. Once detected, it uses a regular expression to extract the color parameter and sends it to the line-following module as the target color, then activates the tracking function. If a text response is generated, the system delivers it through TTS for voice feedback. During execution, it coordinates action handling with the completion status of audio playback. After the task is finished, it resets the status flags and re-enables the voice-wake function, ready for the next command.



## 10.2.5 Color Tracking

### 10.2.5.1 Program Overview

When the program starts, the voice device will announce “**I'm ready**.” To activate the voice device, say the designated wake word **Hello Hiwonder**. Upon successful activation, the voice device will respond with “**I’m here**.” Once activated, voice commands can be issued, for example, “**Follow the red object**.” The terminal displays the recognized command, and the voice device provides a processed response. The robot then autonomously detects the red object using its camera and starts tracking it.

### 10.2.5.2 Preparation

* **Version Confirmation**

Before starting this feature, ensure that the microphone version configuration in the system is correct. For more details, refer to section [10.2.2 Version Confirmation](#anther10.2.2).

* **Configure the Large Model API Key**

Refer to the section [10.2.1 Large Model API Key Setup](#anther10.2.1) to set up the large model key.

### 10.2.5.3 Operation Steps

> [!NOTE]
> 
> * **Command input is case-sensitive and space-sensitive.**
> 
> * **The robot must be connected to the Internet, either in STA (LAN) mode or AP (direct connection) mode via Ethernet.**

1. Click the terminal <img class="common_img" src="../_static/media/chapter_22/section_1/media/image25.png"  /> on the left side of the system interface to open the command line. Enter the command and press **Enter** to disable the app auto-start service.

```bash
sudo systemctl stop start_app_node.service
```

2. Enter the following command and press **Enter** to launch the color tracking feature.

```bash
ros2 launch large_models_examples llm_color_track.launch.py
```

3) When the command line displays the output shown below and announces “**I’m ready**”, the voice device has completed initialization. At this point, say the wake word **Hello Hiwonder**.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image36.png" style="width:700px" />

4) After the program has loaded successfully, the camera feed will appear on screen.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image37.png" style="width:700px" />

5) When the command line shows the output below and the voice device announces “**I’m here**”, the voice device has been successfully activated. The system will begin recording your voice command.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image38.png" style="width:700px" />

6) When the terminal displays the output shown below, it indicates that the voice device has printed the recognized speech. Now, say the command “Follow the red object.”

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image39.png" style="width:700px" />

7) Upon successful recognition by the speech recognition service of a cloud-based large speech model, the parsed command will be displayed under the `publish_asr_result` output in the terminal.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image39.png" style="width:700px" />

8) When the command line displays the output shown below, the cloud-based large language model has been successfully invoked. It processes the user’s command, provides a verbal response, and generates actions corresponding to the semantic meaning of the command.
   
   The response is automatically generated by the large model. Only the semantic accuracy of the reply is guaranteed, while the wording and formatting may vary.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image40.png" style="width:700px" />

9) Then, the robot will detect the red object in its camera feed and begin tracking it in real time.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image41.png" style="width:700px" />

10) To stop the feature, refer to Step 4 to reactivate the voice device again with the wake word to begin a new interaction cycle.

<img class="common_img" src="../_static/media/chapter_22/section_1/media/image42.png" style="width:700px" />

11) To exit the feature, press **Ctrl+C** in the terminal. If the feature does not shut down immediately, press **Ctrl+C** multiple times. If it still fails to exit, open a new terminal window and run the following command to terminate all active ROS processes and related programs.

```bash
~/.stop_ros.sh
```

### 10.2.5.4 Program Outcome

Once the feature is activated, commands can be freely issued, for example, “**Follow the red object**.” The robot uses its camera feed to detect and track the red object. Similarly, commands such as “Follow the blue object” or “Follow the green object” can be used to have the robot detect and track objects of the corresponding colors.

### 10.2.5.5 Program Analysis

* **Launch File Analysis**

The program file is located at: **/home/ubuntu/ros2_ws/src/large_models_examples/large_models_examples/llm_color_track.launch.py**

Define `launch_setup` Function

```python
def launch_setup(context):
    mode = LaunchConfiguration('mode', default=1)
    mode_arg = DeclareLaunchArgument('mode', default_value=mode)

    app_package_path = get_package_share_directory('app')
    large_models_package_path = get_package_share_directory('large_models')

    object_tracking_node_launch = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            os.path.join(app_package_path, 'launch/object_tracking_node.launch.py')),
        launch_arguments={
            'debug': 'true',
        }.items(),
    )

    large_models_launch = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            os.path.join(large_models_package_path, 'launch/start.launch.py')),
        launch_arguments={'mode': mode}.items(),
    )

    llm_color_track_node = Node(
        package='large_models_examples',
        executable='llm_color_track',
        output='screen',
    )

    return [mode_arg,
            object_tracking_node_launch,
            large_models_launch,
            llm_color_track_node,
            ]
```

This function is used to configure and launch actions in a Launch file.

`mode = LaunchConfiguration('mode', default=1)`: Defines a Launch parameter named mode with a default value of 1.

`mode_arg = DeclareLaunchArgument('mode', default_value=mode)`: Declares the mode parameter and adds it to the Launch description.

`objet_sorting_launch`: Uses `IncludeLaunchDescription` to include the `object_tracking_node.launch.py` file from the `large_models` package and passes the mode parameter.

`large_models_launch`: Uses `IncludeLaunchDescription` to include the `start.launch.py` file from the `large_models` package and passes the mode parameter.

`llm_color_track_node`: Defines a ROS2 node from the `large_models` package that runs the `llm_color_track` executable and prints its output to the screen.

Finally, returns a list containing all Launch actions.

* **Python File Analysis**

The program file is located at: **/home/ubuntu/ros2_ws/src/large_models_examples/large_models_examples/llm_color_track.py**

1\. Define Prompt Template (PROMPT)

```python
else:
    PROMPT = '''
**Role
You are an intelligent companion robot. Your job is to generate corresponding JSON commands based on the user’s input.

**Requirements
- For every user input, search the action function library for matching commands and return the corresponding JSON instruction.
- Craft a witty, ever-changing, and concise response (between 10 to 30 characters) for each action sequence to make interactions lively and fun.
- Output only the JSON result — do not include explanations or any extra text.
- Output format:{"action": ["xx", "xx"], "response": "xx"}

**Special Notes
The "action" key holds an array of function name strings arranged in execution order. If no match is found, return an empty array [].
The "response" key must contain a cleverly worded, short reply (10–30 characters), adhering to the tone and length guidelines above.

**Action Function Library
Track an object of a specific color: color_track('red')

**Example
Input: Track a green object
Output:
{"action": ["color_track('green')"], "response": "Got it!"}
    '''
```

2\. Class Initialization

```python
def __init__(self, name):
    rclpy.init()
    super().__init__(name)

    self.action = []
    self.stop = True
    self.llm_result = ''
    # self.llm_result = '{"action": ["color_track(\'blue\')"], "response": "ok！"}'
    self.running = True
    self.action_finish = False
    self.play_audio_finish = False

    timer_cb_group = ReentrantCallbackGroup()
    self.tts_text_pub = self.create_publisher(String, 'tts_node/tts_text', 1)
    self.create_subscription(String, 'agent_process/result', self.llm_result_callback, 1)
    self.create_subscription(Bool, 'vocal_detect/wakeup', self.wakeup_callback, 1, callback_group=timer_cb_group)
    self.create_subscription(Bool, 'tts_node/play_finish', self.play_audio_finish_callback, 1, callback_group=timer_cb_group)
    self.awake_client = self.create_client(SetBool, 'vocal_detect/enable_wakeup')
    self.awake_client.wait_for_service()
    self.set_mode_client = self.create_client(SetInt32, 'vocal_detect/set_mode')
    self.set_mode_client.wait_for_service()

    self.set_model_client = self.create_client(SetModel, 'agent_process/set_model')
    self.set_model_client.wait_for_service()
    self.set_prompt_client = self.create_client(SetString, 'agent_process/set_prompt')
    self.set_prompt_client.wait_for_service()

    self.enter_client = self.create_client(Trigger, 'object_tracking/enter')
    self.enter_client.wait_for_service()
    self.start_client = self.create_client(SetBool, 'object_tracking/set_running')
    self.start_client.wait_for_service()
    self.set_target_client = self.create_client(SetColor, 'object_tracking/set_color')
    self.set_target_client.wait_for_service()

    self.timer = self.create_timer(0.0, self.init_process, callback_group=timer_cb_group)
```

By initializing state parameters, including the action list, stop flag, LLM processing results, and completion status, a TTS text publisher and subscribers are created to receive agent processing results, voice wake signals, and audio playback completion status. Service client requests are also established for voice wake, node configuration, model setup, prompt settings, and object tracking functions, including entering tracking mode, setting run status, and specifying the target color. Finally, a timer is created to start the initialization process.

3\. Set LLM Model

```python
def init_process(self):
    self.timer.cancel()
    msg = SetModel.Request()
    msg.model = llm_model
    msg.model_type = 'llm'
    msg.api_key = api_key 
    msg.base_url = base_url
    self.send_request(self.set_model_client, msg)

    msg = SetString.Request()
    msg.data = PROMPT
    self.send_request(self.set_prompt_client, msg)

    init_finish = self.create_client(Trigger, 'object_tracking/init_finish')
    init_finish.wait_for_service()
    self.send_request(self.enter_client, Trigger.Request())
    speech.play_audio(start_audio_path)
    threading.Thread(target=self.process, daemon=True).start()
    self.create_service(Empty, '~/init_finish', self.get_node_state)
    self.get_logger().info('\033[1;32m%s\033[0m' % 'start')
    self.get_logger().info('\033[1;32m%s\033[0m' % PROMPT)
```

4\. `play_audio_finish_callback` Method

```python
def play_audio_finish_callback(self, msg):
    self.play_audio_finish = msg.data
```

The method handles the callback after audio playback and re-enables the voice wake-up functionality.

5\. `process` Method

```python
def process(self):
    while self.running:
        if self.llm_result:
            msg = String()
            if 'action' in self.llm_result: # If there is a corresponding action returned, extract and process it（If there is a corresponding behavior returned, extract and process it.）
                result = eval(self.llm_result[self.llm_result.find('{'):self.llm_result.find('}')+1])
                if 'action' in result:
                    text = result['action']
                    # Use regular expressions to extract all strings inside parentheses.
                    pattern = r"color_track\('([^']+)'\)"
                    # Use re.search to find the matching result.
                    for i in text:
                        match = re.search(pattern, i)
                        # Extract the result.
                        if match:
                            # Get all argument parts, precisely content inside parentheses.
                            color = match.group(1)
                            self.get_logger().info(str(color))
                            color_msg = SetColor.Request()
                            color_msg.data = color
                            self.send_request(self.set_target_client, color_msg)
                            # Start sorting.
                            start_msg = SetBool.Request()
                            start_msg.data = True 
                            self.send_request(self.start_client, start_msg)
                if 'response' in result:
                    msg.data = result['response']
            else: # If there is no corresponding action, just respond（If there is no corresponding behavior, only respond.）
                msg.data = self.llm_result
            self.tts_text_pub.publish(msg)
            self.action_finish = True
            self.llm_result = ''
        else:
            time.sleep(0.01)
        if self.play_audio_finish and self.action_finish:
            self.play_audio_finish = False
            self.action_finish = False
            # msg = SetInt32.Request()
            # msg.data = 1
            # self.send_request(self.set_mode_client, msg)
            msg = SetBool.Request()
            msg.data = True
            self.send_request(self.awake_client, msg)
            self.stop = False
    rclpy.shutdown()
```

By continuously monitoring the large model output, the system checks for any instruction that contains the `color_track` action. Once detected, it uses a regular expression to extract the color parameter and sends it to the color-tracking module as the target color, then activates the tracking function. If a text response is generated, the system delivers it through TTS for voice feedback. During execution, it coordinates action handling with the completion status of audio playback. After the task is finished, it resets the status flags and re-enables the voice-wake function, ready for the next command.